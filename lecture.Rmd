---
title: "About Last Week's Lab . . . "
output: 
  html_notebook:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: cerule
---


# Setup

```{r setup, include=FALSE}
options(scipen = 999)
library(tidyverse)
diamonds <- read_csv("data/diamonds.csv")
```

# Data

```{r}
diamonds
```

## Columns

A data frame with 53940 rows and 10 variables:

- carat: weight of the diamond (0.2–5.01)
- cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)
- color: diamond colour, from D (best) to J (worst)
- clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))
- depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43–79)
- table: width of top of diamond relative to widest point (43–95)
- price: price in US dollars (\$326–\$18,823)
- x: length in mm (0–10.74)
- y: width in mm (0–58.9)
- z: depth in mm (0–31.8)


# Revisiting Model 1

- Below is our first model from last week.
- Let's discuss what each part of this means.

```{r}
# This is fancy-speak for price as a function of carat.
model_1 <- lm(price ~ carat, data = diamonds)
## Note: This is NOT summarize!
summary(model_1)
```

One thing we didn't do is graph our model. And, well, we should have.

```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
    geom_point() +
    geom_smooth(method = "lm")
```

- Here you can easily see how the coefficient for carat (7756.43) makes sense.
- And you can see where/how our Intercept dips below 0.
- And this was our challenge last week.

And, last week, we assigned the predicted and residual values to our original data. So, let's do that again:

```{r}
diamonds <-
    diamonds %>%
    mutate(
        predicted_1 = model_1$fitted.values,
        residuals_1 = model_1$residuals,
        too_low = if_else(predicted_1 < 0, TRUE, FALSE)
    )
diamonds
```

So, where are our "too low" predictions?

```{r}
ggplot(diamonds, aes(x = carat, y = price, color = too_low)) +
    geom_point(alpha = .1) +
    geom_smooth(method = "lm", color = "blue") +
    scale_color_brewer(palette = "Dark2")
```

- Our model is most obviously wrong for the cheapest diamonds.
- If our goal is to predict the price of a diamond, this could be a problem.
    - Cuz how many people do you know buying 20k ice?

# Lab Challenge

I challenged you to make increase the r-squared value and reduce the number of below 0 prediction. This was, perhaps, not entirely fair.

- Perhaps we can find a pattern.
- Look at our plot above.
    - Do you see any patterns?

What if we did a regression for all diamonds under 1 carat?

```{r}
ggplot(diamonds %>% filter(carat < 1), aes(x = carat, y = price, color = too_low)) +
    geom_point(alpha = .1) +
    geom_smooth(method = "lm", color = "blue") +
    scale_color_brewer(palette = "Dark2")
model_small <- lm(price ~ carat, diamonds %>% filter(carat < 1))
summary(model_small)
```

- This is better.
- Our intercept went from -2256 to -995.
- But, ever here, we see a strong pattern.

```{r}
ggplot(diamonds %>% filter(carat >= 1), aes(x = carat, y = price, color = too_low)) +
    geom_point(alpha = .1) +
    geom_smooth(method = "lm", color = "blue") +
    scale_color_brewer(palette = "Dark2")
model_big <- lm(price ~ carat, diamonds %>% filter(carat >= 1))
summary(model_big)
```

Perhaps we can create a new variable to help the model see this banding.

- And remember, this is done by eye-ball.

```{r}

diamonds <-
    diamonds %>%
    mutate(carat_group = case_when(
        carat < 0.3 ~ "< 0.3",
        carat >= 0.3 & carat < 0.5 ~ "0.3 - 0.4",
        carat >= 0.5 & carat < 0.7 ~ "0.5 - 0.7",
        carat >= 0.7 & carat < 0.9 ~ "0.7 - 0.8",
        carat >= 0.9 & carat < 1.0 ~ "0.9 - 0.99",
        carat >= 1.0 & carat < 1.5 ~ "1.0 - 1.4",
        carat >= 1.5 & carat < 2.0 ~ "1.5 - 1.99",
        carat >= 2.0 & carat < 3.0 ~ "2.0 - 2.99",
        carat >= 3.0 ~ "> 3"
    ))
ggplot(diamonds, aes(x = carat, y = price, color = too_low)) +
    geom_point(alpha = .1) +
    geom_smooth(method = "lm", color = "blue") +
    scale_color_brewer(palette = "Dark2") +
    facet_wrap(~carat_group)
model_complicated <- lm(price ~ carat + carat_group, diamonds)
summary(model_complicated)
```

So, how did we do?

```{r}
diamonds <-
    diamonds %>%
    mutate(
        predicted_complicated = model_complicated$fitted.values,
        residuals_complicated = model_complicated$residuals,
        too_low = if_else(predicted_complicated < 0, TRUE, FALSE)
    )
diamonds %>% filter(too_low)
```

- This is what #winning looks like.

```{r}
ggplot(diamonds, aes(x = predicted_1)) +
    geom_density() +
    labs(title = "Model 1: Distribution of Predicted Values")

ggplot(diamonds, aes(x = predicted_complicated)) +
    geom_density() +
    labs(title = "Model Complicated: Distribution of Predicted Values")
```

PLEASE notice the difference!

```{r}
diamonds %>%
    summarize(
        model_1_mean_residuals = mean(residuals_1),
        model_1_sd_residuals = sd(residuals_1),
        model_c_mean_residuals = mean(residuals_complicated),
        model_c_sd_residuals = sd(residuals_complicated),
    )
ggplot(diamonds, aes(x = residuals_1)) +
    geom_density() +
    labs(title = "Model 1: Distribution of Predicted Values")

ggplot(diamonds, aes(x = residuals_complicated)) +
    geom_density() +
    labs(title = "Model Complicated: Distribution of Predicted Values")
```

As you can see, our residuals are closer to zero, but more importantly, the standard deciation is down by over $100.

```{r}
ggplot(diamonds, aes(x = price, y = predicted_1)) +
    geom_point() +
    labs(title = "Model 1: Price Compared to Predicted Price")
ggplot(diamonds, aes(x = price, y = predicted_complicated)) +
    geom_point() +
    labs(title = "Model Complicated: Price Compared to Predicted Price")
```

- This (hopefully) teaches us something important about modeling data.
    - model_1 has an adjusted r-squared of ~.85. 
    - model_complicated is only .87.
    - So, yes, better, but not wildly so.
    - Yet, model_complicated is a better, more practical model.
        - It isn't actually all that complicated.
        - It produces no results which are OBVIOUSLY stupid.
            - (At this level of obvious.)
    - We also have a saner looking estimate for the slope for carat.

```{r}
model_more_complicated <- lm(price ~ carat * carat_group, diamonds)
summary(model_complicated)
summary(model_more_complicated)
```

- There are plenty of advanced techniques to understand a model.
- And those are important but . . . 
- When common sense is excluded from model assessment, stupid things happen.