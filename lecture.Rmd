---
title: "Linear Regression"
output: 
  html_notebook:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: cerule
---

# Linear Regression

- This is where we predict the value of a continuous variable based on other values in our data.
- Last week we observed a correlation between age and charges in our insurance data.



# Setup

```{r setup, include=FALSE}
library(rio)
library(tidyverse)
library(modelr)
options(scipen = 999)

insurance <- import("data/insurance.csv")
```



# Data

A data frame with 1,338 rows and 7 variables which we will use to model health insurance cost next week. This week we will explore the data to better understand it.
```{r}
insurance
```

## Columns

- age: age of primary beneficiary
- sex: insurance contractor gender, female, male
- bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,
objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
- children: Number of children covered by health insurance / Number of dependents
- smoker: Smoking
- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
- charges: Individual medical costs billed by health insurance

Data Source: https://www.kaggle.com/datasets/mirichoi0218/insurance



# Last Week's Lab

I hope you remember these plots from last week:

```{r}
ggplot(insurance, aes(x = charges)) +
    geom_density() +
    theme_minimal()
```

- The distribution of charges has a strong positive skew because of the long tail of the values.
- We can see that there is a positive relationship between age and charges.
  - On average, older members have higher charges.

```{r}
ggplot(insurance, aes(x = age, y = charges)) +
    geom_point() +
    geom_smooth(method = "lm") +
    theme_minimal()
```



# Simple Age Model

- Let's build a linear model of this relationship.
- Formula: charges ~ bmi
- This is a Base R function which makes it hard to pipe data into the function.

```{r }
age_model <- lm(charges ~ age, data = insurance)
summary(age_model)
```

Coefficients:

- Intercept: The estimated value of y when x is zero.
  - Example: When age is 0, this model predicts the cost will be $3,165.90.
  - Remember: Our data only contains members ages 18-64. Therefore we should only use this model for that age range. But there is still a y-intercept value.
- age: For every unit increase in x, how much does y increase/decrease?
  - In this case, cost increases $257.70 for every year the member ages.
- Adjusted R-squared: How much of the variance in our data is explained by the model? Will have a value between 0 and 1, higher is better.
  - This isn't particularly good.
- p-value: Is the model statisticall significant. <= .05
  - We will discuss this concept more, but this will do for right now.
- This object, `age_model`, has a series of things we need to discuss:

```{r}
# Column names of our data!
names(insurance)

# Things in our model!
names(age_model)
```

- fitted.values: Values predicted by our model.
- residuals: Difference between the actual value and our predicted value.

The code chunk below proves that these are the same.

```{r}
# Adds our predicted values as a new column.
insurance <-
    insurance |>
    add_predictions(age_model)

# The thick blue line is what we did before with ggplot.
# The green line is the model created by lm above.
ggplot(insurance, aes(x = age, y = charges)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, size = 5) +
    geom_line(aes(y = pred), color = "green", size = 2)
theme_minimal()
```

- **Question:** Why is this age_model more useful than the ggplot line?
- **Answer:** We can do more with it!

We can use our model object to evaluate the model and to make new predictions!

```{r}
new_members <- tibble(
    age = c(10, 20, 25, 30, 35, 40, 100)
)
new_members |>
    add_predictions(age_model)
```

BE VERY CAREFUL PREDICTING WITH VALUES BEYOND YOUR TRAINING DATA!!!!!

And we can evaluate out model:
```{r}
insurance <-
    insurance |>
    mutate(residuals = charges - pred)

print("For age_smoker_model")
insurance |>
    summarize(
        avg_residual = mean(residuals),
        sd_residuals = sd(residuals)
    )

ggplot(insurance, aes(residuals)) +
    geom_density()
```

- Remember how our model only explains 8-9 percent of the variance in the data?
- This is why. Our residuals should be normally distributed.
- This model isn't very good.



# Age + Smoker Model

- Remember this observation?
- Non-smokers cost less, regardless of age.

```{r}
# Removes the predicted/residual values from our last model.
insurnace <- insurance |> select(-pred, -residuals)

ggplot(insurance, aes(x = age, y = charges, color = smoker)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    theme_minimal()
```

Perhaps we can use lm to build a model like this!

```{r}
age_smoker_model <- lm(charges ~ age + smoker, data = insurance)
summary(age_smoker_model)
```

- Right off the bat, we've done something AWESOME.
- Adjusted R-squared when from 0.08 to .72. That is a 9x improvement.
- But what did we do???

We have a some more complexity to our model.

```{r}
tibble(
    age = c(0, 0, 45, 45),
    smoker = c("no", "yes", "no", "yes")
) |>
    add_predictions(age_smoker_model)
```

We now have two intercepts and thus two lines.

- Intercept for non-smokers is -2391.63
- Intercept for smokers is 23855.30 + -2391 which is 21463.67!
- In this instance, the slope is the same. For every year older, +274.87

```{r}
insurance <-
    insurance |>
    add_predictions(age_smoker_model)

insurance <-
    insurance |>
    mutate(residuals = charges - pred)

print("For age_smoker_model")
insurance |>
    summarize(
        avg_residual = mean(residuals),
        sd_residuals = sd(residuals)
    )

ggplot(insurance, aes(residuals)) +
    geom_density()
```

- And now you can see that this is MUCH better model.
- Our residuals are much closer to 0.
- And we have a reasonable number of values on either side of 0.
- This isn't a great model. But it is . . . say 9x better than it was!